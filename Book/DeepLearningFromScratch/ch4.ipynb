{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4절 이전 함수들 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 소프트맥스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평균 제곱 오차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 교차 엔트로피 오차 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error_legacy(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.11809565095832"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
    "y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "cross_entropy_error_legacy(y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error_non_delta(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python35\\lib\\site-packages\\ipykernel\\__main__.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "c:\\python35\\lib\\site-packages\\ipykernel\\__main__.py:7: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "cross_entropy_error_non_delta(y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + delta)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51082545709933802"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "y = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "cross_entropy_error(y, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mask 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "a = np.array([])\n",
    "for i in range(10000):\n",
    "    a = np.append(a, np.random.choice(60000, 100))\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "b = np.unique(a)\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 미분 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function_2(x): # x는 배열\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- numerical_gradient 구현/테스트\n",
    "- ch04/gradient_2d.py 확인\n",
    "- 4.4.1 경사하강법, 공식\n",
    "- gradient_descent 구현/테스트\n",
    "- ch04/gradient_method.py 확인\n",
    "- 4.4.2 기울기, ch04/gradient_simplenet.py\n",
    "- 이상한 더미인수 W와 net.loss(x, t)\n",
    "- 4.5 학습알고리즘 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에서는 $x_{0}$랑 $x_{1}$의 편미분을 별도의 함수 2개로 구분해 따로 계산했다.  \n",
    "그럼 $x_{0} = 3$, $x_{1} = 4$일 때 양쪽의 편미분을 묶어서 $(\\frac{\\partial{f}}{\\partial{x_{0}}}, \\frac{\\partial{f}}{\\partial{x_{1}}})$ 을 계산한다 해보자.  \n",
    "이 때 $(\\frac{\\partial{f}}{\\partial{x_{0}}}, \\frac{\\partial{f}}{\\partial{x_{1}}})$처럼 모든 변수의 편미분을 벡터로 정리한 것을 **기울기 gradient**라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_gradient_1d(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for i in range(x.size):\n",
    "        a, b = x.copy(), x.copy()\n",
    "        a[i] = a[i] + h\n",
    "        b[i] = b[i] - h\n",
    "        grad[i] = (f(a) - f(b)) / (2*h)\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 다차원 배열도 사용 가능하게 만든 버전\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.  8.]\n",
      "[ 0.  4.]\n",
      "[ 6.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_gradient_1d(function_2, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient_1d(function_2, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient_1d(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경사하강법 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, a=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x) # grad는 x에서의 기울기\n",
    "        x -= a * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -6.11110793e-10,   8.14814391e-10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(\n",
    "    function_2,\n",
    "    init_x=np.array([-3.0, 4.0]),\n",
    "    a=0.1,\n",
    "    step_num=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망에서의 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00555553  1.02163482  0.62235991]\n",
      " [ 0.60798662  0.69505252 -0.02276852]]\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.15052128,  1.23852816,  0.35292428])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차가 작음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7307071641347185"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1])\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오차가 높음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93311048023807153"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = np.array([1, 0, 0])\n",
    "net.loss(x, t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기를 구해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.23599683  0.2577075  -0.49370433]\n",
      " [ 0.35399524  0.38656126 -0.7405565 ]]\n"
     ]
    }
   ],
   "source": [
    "# 책 버전 함수의 인수W가 더미임\n",
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.23599683  0.2577075  -0.49370433]\n",
      " [ 0.35399524  0.38656126 -0.7405565 ]]\n"
     ]
    }
   ],
   "source": [
    "# 위 함수를 풀어쓴 것\n",
    "def f1(W):\n",
    "    z = np.dot(x, W)\n",
    "    y = softmax(z)\n",
    "    loss = cross_entropy_error(y, t)\n",
    "    return loss\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 알고리즘 구현\n",
    "신경망 <- 가중치, 편향  \n",
    "학습 == 가중치, 편향이 훈련 데이터에 적응하도록 조정하는 과정  \n",
    "학습 == 4단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1단계 : 미니배치\n",
    "훈련 데이터의 일부를 무작위로 가져옴 -> 선별한 데이터 == 미니배치  \n",
    "목적 : 미니배치에 대한 손실 함수 값을 줄이는 것\n",
    "### 2단계 : 기울기 계산\n",
    "가중치의 기울기를 구함  \n",
    "기울기 == 손실 함수의 값을 작게 하는 방향을 제시\n",
    "### 3단계 : 갱신\n",
    "가중치를 기울기 방향으로 아주 조금 갱신\n",
    "### 4단계 : 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 우리는 확률적 경사 하강법(SGD)으로 신경망을 학습시킨다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2층 신경망 클래스 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        #print(\"■\",end=\"\")\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        #print(\"■\",end=\"\")\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        #print(\"■\",end=\"\")\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        #print(\"■\",end=\"\")\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        #print(\"■\",end=\"\")\n",
    "        \n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        #print(\"■\",end=\"\")\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        #print(\"■\",end=\"\")\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        #print(\"■\",end=\"\")\n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "        #print(\"■\",end=\"\")\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(\n",
    "    input_size=784,\n",
    "    hidden_size=100,\n",
    "    output_size=10\n",
    ")\n",
    "print(net.params[\"W1\"].shape)\n",
    "print(net.params[\"b1\"].shape)\n",
    "print(net.params[\"W2\"].shape)\n",
    "print(net.params[\"b1\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 784), (100, 10))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "y = net.predict(x)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True, flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 다차원 배열도 사용 가능하게 만든 버전\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        # print(\".\",end=\"\")\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 2.28415047754\n",
      "loss : 2.2832540934\n",
      "loss : 2.26898039972\n",
      "loss : 2.23016260837\n",
      "loss : 2.10615165591\n",
      "loss : 1.97851439661\n",
      "loss : 1.66866295012\n",
      "loss : 1.49219938285\n",
      "loss : 1.23622290641\n",
      "loss : 1.15032506146\n",
      "loss : 1.02216141335\n",
      "loss : 0.950289649009\n",
      "loss : 0.761099162067\n",
      "loss : 0.720386930532\n",
      "loss : 0.686802349532\n",
      "loss : 0.608701121435\n",
      "loss : 0.708324175823\n",
      "loss : 0.581166926263\n",
      "loss : 0.57992923006\n",
      "loss : 0.475084431594\n"
     ]
    }
   ],
   "source": [
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "# print(\"network 생성\")\n",
    "# 하이퍼파라미터\n",
    "iters_num = 1000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "# print(\"하이퍼 파라미터 생성 완료\")\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    # print(\"배치 획득 완료 -> \",end=\"\")\n",
    "    # 기울기 계산\n",
    "    # grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    # print(\"기울기 계산 완료 -> \",end=\"\")\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    # print(\"갱신 완료 -> \",end=\"\")\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    # print(\"기록 완료\")\n",
    "    if i % 50 == 0:\n",
    "        print(\"loss :\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600.0\n",
      "train acc, test acc | 0.104416666667, 0.1028\n",
      "train acc, test acc | 0.787066666667, 0.7899\n",
      "train acc, test acc | 0.875, 0.8796\n",
      "train acc, test acc | 0.895483333333, 0.8983\n",
      "train acc, test acc | 0.908333333333, 0.9113\n",
      "train acc, test acc | 0.9135, 0.9153\n",
      "train acc, test acc | 0.918383333333, 0.9215\n",
      "train acc, test acc | 0.922983333333, 0.9236\n",
      "train acc, test acc | 0.926583333333, 0.9276\n",
      "train acc, test acc | 0.929216666667, 0.9296\n",
      "train acc, test acc | 0.932616666667, 0.9331\n",
      "train acc, test acc | 0.934766666667, 0.9349\n",
      "train acc, test acc | 0.936733333333, 0.9373\n",
      "train acc, test acc | 0.939383333333, 0.9394\n",
      "train acc, test acc | 0.941833333333, 0.9411\n",
      "train acc, test acc | 0.943416666667, 0.9421\n",
      "train acc, test acc | 0.944966666667, 0.943\n"
     ]
    }
   ],
   "source": [
    "network2 = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "print(iter_per_epoch)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    # grad = network2.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network2.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network2.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network2.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network2.accuracy(x_train, t_train)\n",
    "        test_acc = network2.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZE2/w9zGS0aQwLYzAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1920aa93198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
