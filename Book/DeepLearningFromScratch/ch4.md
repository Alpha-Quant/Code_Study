# 챕터 4 : 신경망 학습

`훈련 데이터`와 `시험 데이터`를 나눈다. -> `오버피팅`된 모델을 피하고 `범용 능력`을 제대로 평가하기 위해

## 손실 함수 loss function
비용 함수 cost function 라고도 함

### 평균 제곱 오차 mean squared error, MSE
수식 어떻게 쓰는지 모르겠네  

> [여기서 함](http://latex.codecogs.com/eqneditor/editor.php)  

![오 어떻게 하는지 알았다](img/ch4_식4.1.png)

직관적이당  
한 원소만 1로 그 외는 0 -> `원-핫 인코딩`  

```python
def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2)
```

오차가 더 작은 쪽이 정답에 더 가까울 것이라고 판단 가능

### 교차 엔트로피 오차 cross entropy error, CEE
이걸 더 자주 본 듯

![직관적이지가 않아](img/ch4_식4.2.png)

> 별로 직관적으로 와닿진 않는데 `딥러닝 첫걸음`이라는 책에는 좀 더 친절하게 설명되어 있다.  
그 책의 설명을 가져와 보면 신경망의 출력 y가 0에서 1사이여야 하기 때문에 시그모이드, 소프트맥스 활성함수와 같이 사용되는 경우가 많다.  
직관적으로 이해해 보려면 그냥 오차 d-y(딥러닝 첫걸음에서는 정답이 d이다)가 0일 때는 비용함수의 값이 0이고 y가 0에 가까울수록, 즉 오차가 커질수록 비용함수의 값이 급격하게 커진다. (log니까)

하여튼 수식의 설명은 이렇게 얘는 `MSE`보다 같은 오차에 더 민감하게 반응한다는 점이다.  
그래서 회귀 문제와 같은 경우가 아니면 `CEE`가 좋다고 한다.

```python
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```
`delta`가 존재하는 이유는 `np.log()`에 0을 전달하면 `-inf`가 튀어나오기 때문에 그렇다.

### 미니배치 학습
#### 미니배치의 필요성
기계학습은 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾아낸다.  
위에서 본 손실 함수는 훈련 데이터 하나에 대한 것이고, 훈련 데이터가 100개라면 손실 함수로 계산한 100개의 값들을 합해 지표로 삼는다.  
그러려면 그냥 위의 수식에서 앞에 `1/n * sigma` 해주면 된다.  
수식은 만들기 귀찮아서 생략한다. 하여튼 이렇게 `평균 손실 함수`를 구할 수 있다.  

MNIST 데이터셋은 훈련 데이터가 60,000개다. (내가 알기론 5만개 + 테스트용 1만개인데 그게 그거니 넘어가자)  
다른 학습 모델을 위해 훈련시킬 땐 데이터가 수백만 수천만도 넘는다. -> 학습시간 폭발  
그렇기에 훈련 데이터 중 일부를 추려 전체의 `근사치`로 이용한다.  
이렇게 일부만 골라 학습을 수행하는데 그 `일부`를 `미니배치 mini-batch`라고 한다.  
6만장중 100장을 무작위로 뽑아 학습시키는 것

#### 미니배치 코드

```python
import sys, os
sys.path.append(os.pardir) # 이부분은 3장에 설명되어있다.
import numpy as np
from dataset.minist import load_mnist # 이부분도 3장에 설명되어있다.
# 추후에 시간이 있으면 load_mnist에 대해 공부한 내용을 올릴 것이다

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
# normalize가 True면 각 픽셀의 값(여기선 784개의 열 각각)이 0.0과 1.0사이다
# one_hot_label이 True면 t_train(정답)의 값이 [0, 1, 0, ...]식으로 된다.
# False이면 그냥 3, 5, 1, 9 이런식으로 된다.

print(x_train.shape) # (60000, 784)
print(t_train.shape) # (60000, 10)
```
훈련 데이터가 6만개인게 맞구나 하핫  
`ndarray`객체라서 shape프로퍼티가 있다.  
행이 6만개 열이 784개인데 손글씨 이미지가 28x28해상도이다.  
이를 1차원으로 쭉 펼쳐놔서 784인 것이다.  
`t_train`은 열이 10개인데 0부터 9까지 정답인 열이 1의 값을 갖는다.

여기서 무작위로 10장만 빼내려면

```python
train_size = x_train.shape[0] # 60000
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```

`np.random.choice(60000, 10)`은 0과 60000사이에서 10개를 뽑아 `ndarray`로 반환한다.
```python
>>> np.random.choice(60000, 10)
array([8013, 23832, ...])
```
그래서 이 mask를 인덱스에 전달하면 해당 인덱스에 해당하는 훈련 데이터만 뽑혀나온다.  
역시 갓갓 numpy!

> 시뮬레이션을 해봤는데 중복된 수가 (당연히)뽑혀나온다.  
`np.random.choice(60000, 100)`을 600번 수행해봤는데 중복된 수를 제외하면 약 37000개 밖에 안된다.  
이를 어떻게 처리했는지는 뒤에가서 두고보자. 만약 처리를 안해주었다면 검증용 mask를 만들어서 해당 mask가 True든 False든 하나로 통일될 때 까지 반복하면 된다.

### 미니배치용 교차 엔트로피 오차
아까랑 구현이 미묘하게 다르다.  
아까꺼가 보기 귀찮아서 여기로 가져왔다.

```python
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```
이제 이걸 이렇게 바꿨다.

```python
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    
    batch_size = y.shape[0]
    return -np.sum(t * np.log(y)) / batch_size
```
아까 위에서 `x_batch`, `t_batch`를 뽑았는데 한 20개 뽑았다고 해보자.  
그럼 `x_batch.shape`는 `(20, 784)`, `t_batch.shape`는 `(20, 10)`이다.  
그리고 어찌저찌 신경망을 통하고 softmax 활성함수를 통해  
`[0.0, 6.0, 1.0, 0.5, 0.0, 1.0, 0.5, 2.0, 0.0, 0.0]`의 y 하나를 구할 수 있다. 소프트맥스에 관한건 3장에 있다.  
20개를 뽑았으니 y의 shape는 `(20, 10)`이다.  
그럼 if문은 가볍게 무시되고 `batch_size`는 20이 된다.  
t는 one hot encoding되어있기에 0인 부분의 로그곱도 가볍게 무시되고 t가 1인 부분만 `np.log(y)`가 실행된다.  
그리고 `np.sum()`을 통해 20개의 행을 다 합해주고 `batch_size`로 나누어 최종적으로 `평균 CEE`를 구한다.

만약 `y.ndim`이 1일 때, 즉 `batch_size`가 1인 경우, 다른 말로 데이터가 하나일 경우에는 `reshape()`로 데이터의 형태를 다듬는다.  
이렇게 해주지 않으면 `y.shape[0]`이 10(열의 개수)가 되어버려서 의도했던 출력이 나오지 않는다.

원-핫 인코딩이 False라 2나 7등의 숫자 레이블로 t가 주어졌다면 구현이 좀 다르다.
`t * np.log(y)`부분이 `np.log(y[np.arange(batch_size), t])`로 바뀐다.
> `batch_size`를 5라고, t는 `[2, 7, 0, 9, 4]`라고 가정해보자.  
`np.arange(batch_size)`는 `[0, 1, 2, 3, 4]`가 된다.  
그러므로 `y[np.arange(batch_size), t]`는 `y[[0, 1, 2, 3, 4], [2, 7, 0, 9, 4]]`이다.  
y의 각각의 행(총 5개)에 대해 정답인 열(y의 열은 총 10개)만 뽑혀서 곧 `y[0,2]`, `y[1, 7]`의 값에 대해서만 np.log를 실행하게 된다.

### 손실 함수의 필요성
그냥 총 정확도만 따지면 되는 것 아닐까? (정답 갯수 / 훈련 데이터 갯수 처럼)  
책에서는 `미분`의 역할에 주목하라 한다.  
학습에서 최적의 가중치를 찾을 때 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾는다. -> 오키 이해감  
매개변수의 미분(기울기)을 계산하고, 그걸 단서로 매개변수를 서서히 갱신한다. -> 어떻게 단서로 쓰지?  
`가중치의 손실 함수의 미분`이란 `가중치 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하나`라는 의미 -> 아항, 손실함수가 커지는 방향으로 안가게 하려는 거구나  
미분 값이 음수면 가중치를 양의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다.

> 봉우리 같은 2차원 그래프를 생각해보자.  
∩처럼 생긴 가중치-손실함수 그래프가 있는데 여기서 기울기가 음수가 나왔다 치자.  
(봉우리 오른쪽에서 점을 찍었다고 생각하자)  
여기서 가중치(x축의 값)를 양의 방향으로 이동시키면 손실함수(y축의 값)의 값이 줄어든다.

정확도를 지표로 삼으면 매분 값이 대부분의 장소에서 0이 된다. -> 오잉?  
예로 100장중 32장을 올바르게 인식했다고 하자.  
그럼 32%일텐데 여기서 가중치를 약간 조정했다쳐도 그대로 32%일 확률이 높다.  
개선된다 쳐도 31%, 33%처럼 불연속적인 띄엄띄엄한 값이 될 것이다. -> 말하자면 계단 함수  
하지만 CEE를 사용한 손실 함수의 값은 연속적으로 민감하게 변하게 된다.  
계단함수는 대부분의 장소(0이외의 곳)에서 미분값이 0이다.  
계단함수를 손실 함수로 사용한다면 매개변수의 작은 변화가 주는 파장을 계단 함수가 우걱우걱 무시해버린다.  
그래서 손실 함수의 값에는 아무런 변화가 안나타나게 된다.  

정리하면 정확도를 지표로 삼으면 계단함수를 손실함수로 사용하는 것이고  
이는 대부분의 장소에서 미분값이 0이기 때문에 가중치 값 갱신에 도움이 안된다.  
그래서 연속적인 함수(CEE라든가 시그모이드함수)를 학습에 사용하는 것이다.

## 수치 미분
미분 값, 즉 기울기(경사)값을 기준으로 나아갈 방향을 정한다.  
`미분`을 복습해보자.

### 미분
10분동안 2km를 달렸다 하자.  
미분은 '특정 순간'의 변화량을 뜻한다.  
그래서 10분이라는 시간을 가능한 한 줄여(1분 -> 1초 -> 0.1초 ...) 한 순간의 변화량을 얻는 것이다.  
![미분](img/ch4_식4.4.png)
> x의 '작은 변화'가 함수 f(x)를 얼마나 변화시키느냐를 의미

파이썬으로 구현해보자
참고로 numerical_diff는 `수치 미분 numerical differentiation`에서 따왔다.
```python
# 나쁜 예
def numerical_diff(f, x):
    h = 10e-50
    return (f(x + h) - f(x)) / h
```
나쁜 점 두개
- h에 가급적 작은 값을 대입하고 싶었으나 `반올림 오차 rounding error`문제를 일으킨다.  
(이것에 대해선 예전에 블로그에 포스팅 했었다. [보러가기](http://winterj.me/Floating-Point/))  
그래서 h의 값으로 1e-4정도를 사용한다.
- h의 작지 않은 값(1e-4도 작지 않다)때문에 한 점에서의 접선의 기울기가 아닌 (x + h)와 x사이의 기울기를 구하는게 되어버린다.  
그래서 진정한 미분과 위의 구현은 일치하지 않는다.

두번째 문제를 해결하기 위해 (x+h)와 (x-h)사이의 기울기를 구하면된다. 이를 `중앙 차분`이라고 한다.
```python
# 좋은 예
def numerical_diff(f, x):
    h = 1e-4 # 0.0001
    return (f(x + h) - f(x - h)) / (2 * h)
```

> 이렇게 아주 작은 차분(h)로 미분을 구하는 것이 수치 미분이다.  
우리가 학교에서 수식을 전개해 미분을 구하는 것은 해석적으로 미분을 구한다고 표현한다.  
해석적 미분은 오차가 없는 '진정한 미분'값을 구해준다.

### 수치 미분의 예
y = 0.01x² + 0.1x 인 2차 함수를 미분해보자.
```python
def function_1(x):
    return 0.01*x**2 + 0.1*x
```
이 함수를 그려보자.
```python
import numpy as np
import matplotlib.pylab as plt
# matplotlib.pyplot 을 import해도 된다.
# pylab은 interactive 환경에서 선호되고 pyplot은 스크립팅 환경에서 선호된다.
# 근데 막상 jupyter에서는 pyplot을 써도 된다.
# ipython을 사용할 때는 ipython --pylab을 써야하는걸로 알고있다. 자세한건 링크로~
# http://stackoverflow.com/questions/11469336/what-is-the-difference-between-pylab-and-pyplot

x = np.arange(0.0, 20.0, 0.1)
# 0에서 20까지 0.1간격, 총 200개
y = function_1(x)
# x에 대응되는 y값이 계산된다.
plt.xlabel("x")
plt.ylabel("f(x)")
# 보기 좋게 라벨 붙이기
plt.plot(x, y)
plt.show()
```

<img src="img/ch4_그림4.6.png" alt="그래프" style="width: 300px;"/>  

> 인라인 마크다운에서는 이미지 사이즈 조절이 안된다.

```python
>>> numerical_diff(function_1, 5)
0.1999999999990898
>>> numerical_diff(function_1, 10)
0.2999999999986347
```

저 2차함수의 해석적 미분은 0.02x + 1인데 위의 결과랑 비교해서 보면 무시해도될 오차를 제하면 거의 같은 값이다.

### 편미분
*f(x0, x1) = x0² + x1²*  
이런 함수가 있다고 합시다. 앞과는 달리 변수가 2개네요  
파이썬으로는  
```python
def function_2(x): # x는 배열
    return x[0]**2 + x[1]**2
    # 또는 np.sum(x**2) <- 개인적으로 이걸 선호
```
3차원 그래프로 그려보면... 귀찮아서 생략하지만 그물이 아래로 쳐진 모양이다  
이제 미분해봅시다  
근데 변수가 2개니까 '어느 변수에 대한 미분이냐'를 구별해야함  
이렇게 변수가 여럿인 함수에 대한 미분을 `편미분`이라고 한다.  
대학와서 수학과목을 거의 안들어서 정확히는 모르겠는데 (만약 4장 발표를 한다면 더 조사할 예정)  
나머지 변수를 다 고정시켜두고 미분하는거라한다.  

예를 들어 x0 = 3, x1 = 4일 때 x0에 대한 편미분을 구한다치면  
```python
def function_tmp1(x0):
    return x0*x0 + 4.0**2

numerical_diff(function_tmp1, 3.0)
# 출력은 6.0000000000378 정도
```
참고로 `numerical_diff`의 코드는 아래와 같다.
```python
def numerical_diff(f, x):
    h = 1e-4 # 0.0001
    return (f(x + h) - f(x - h)) / (2 * h)
```

또 x0 = 3, x1 = 4일 때(아까와 같다) x1에 대한 편미분은  
메차쿠차해서 이거다 하려 했는데 코드로 적어보면  
```python
def function_tmp2(x1):
    return 3.0**2 + x1*x1

numerical_diff(function_tmp2, 4.0)
# 출력은 7.9999999999119 정도
```

이렇듯 앞에서는 x1 = 4로 고정된 새로운 함수를 정의하고 변수가 x0하나인 함수에 미분했다.  
즉 편미분은 변수가 하나인 미분과 마찬가지로 측정 장소의 기울기를 구하는데, 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정하는 것이다.  
> 음 개인적으로 이해한건 구분구적법처럼 어느 한순간의 단면(다른 변수 고정)을 가지고 와서 목표 변수에 따른 특정 기울기를 구한다고 생각했다.  
말하자면 이런식  
![편미분이라네](img/ch4_편미분참고그림.jpg)


### 4.4 기울기 부터는 jupyter로 넘어갑니다~~